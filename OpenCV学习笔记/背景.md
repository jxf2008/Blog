在安防领域，有一个常见的需求，当一个物体进入摄像头的范围内，相关软件必须识别出有物体进入。从视频中提取出两帧，其中一帧多了一个物体，比如一辆刚驶入画面的汽车，通常，最大的麻烦不少对驶入的汽车的识别，而是一直存在于画面的背景的干扰。

关于背景和前景，一般来说，背景是一直存在于画面中的物体，比如一棵树，一面墙，而这些物体是需要忽略的，而前景则是被关注的部分。在图像分析中，这两者之间没有绝对的区别，而其中最重要的就是消除背景的影响。比如一个对着大门的摄像头，范围内有一棵柳树，在较大风的情况下，树枝的摇动会导致树枝出现在屏幕的大部分，而这个摄像头的任务是识别是否有车辆进入大门，那么软件要根据画面判断是否有物体在移动（驶入的汽车）就变的非常麻烦。

背景的消除有很多方法，但背景处理的实际情况非常多且具有不确定性，比如某个对着一段围墙的摄像头，本职任务是监测是否有人翻墙入内，该摄像头检测到墙的边缘有物体移动时就会发出通知，结果附近的狸花猫在墙上走过时每次都会导致摄像头误判。因此，对于背景的消除识别与前景的提取需要根据实际情况选择不同的方法或者学习模型，而不存在一直普遍适合的方案。

## 帧间差分

帧间差分是一种简单，实用的方法，当然缺点也很明显。帧间差分的基本原理就是比较两帧，将这两帧的每个对应的元素相减，如果有物体进入画面内，这回导致这个位置的元素，在两帧画面内差距非常大，根据这个原理很容易将进入画面的物体识别出来，opencv提供了函数用于将两个画面相减
```c++
void absdiff(InputArray src1, InputArray src2, OutputArray dst);
```
参数src1和src2是对应的两帧画面，参数dst是两帧相减的结果

下面是用手机在相同位置拍摄的画面，一个画面上有一个耳机，另一个没有。这刚好可以看出一个视频的两帧，一帧有物体进入，而另一种没有。手机拍摄位于相同的位置，但两次拍摄因为是手拿手机，因此拍摄位置不是绝对相同，这和实际的安防摄像头类似，大风，来往车辆震动等会导致摄像头有一定的微调。
```c++
    cv::Mat mat_background1 = cv::imread("E:/BackGround/background1.png");
	cv::Mat mat_background2 = cv::imread("E:/BackGround/background2.png");
	cv::Mat mat_dst;

	cv::cvtColor(mat_background1, mat_background1, CV_8UC1);
	cv::cvtColor(mat_background2, mat_background2, CV_8UC1);

	cv::absdiff(mat_background1, mat_background2, mat_dst);
    //对比结果，如果差距过大，比如大于100，就认为是进入的物体导致的
	cv::threshold(mat_dst,mat_dst, 100, 255, cv::THRESH_BINARY);

	cv::imshow("图一", mat_background1);
	cv::imshow("图二", mat_background2);
	cv::imshow("对比结果", mat_dst);
```
运行结果

![图1](https://jxf2008-1302581379.cos.ap-nanjing.myqcloud.com/github_blog/opencv/background_1.png)

这里可以看出帧间差分很准确的找出了耳机，并且消除了绝大部分的背景。但帧间差分的缺点也很明显，图中耳机放在桌子上，而桌子上有一条细缝，这属于尖锐的噪声，而由于拍摄的位置并不是完全一致，这导致这些尖锐的噪声在两帧里的位置有所偏移，帧间差分就会把这种尖锐的噪声识别为进入画面的物体，而实际情况中，这种尖锐的噪声是非常常见的现象。

## 平均背景法

消除背景的另一个常见的方法是均背景法，这个方法需要一定量的样本，并建立一个学习模型。基本的思路是这样的，对于一个画面，其中很多背景存在一定的变化，但变化的量是有限的，比如墙壁的颜色在正午和傍晚会有所不同，但变化不会太大，因此如有足够的样本统计墙的颜色变化，那当由物体进入墙范围，导致墙的部分颜色发生改变时，就可以通过模型判断，如果变化没有超过样本的范围，可以认为是正常变化，如果变化范围超过了之前模型统计的范围，那就可以认为是有物体进入了。

为了统计样本，需要建立一个简单的学习模型，如下
```c++
//这个模型根据不同的视频长度和硬件，执行的时间可能会有较大的差别，以我的电脑为例，一段手机拍摄的10秒的视频，需要超过3分钟才能全部运行完成，但使用opencv示例中的视频，仅需要几秒就可以完成
class BackGroundStudy {
private:
	float thres_high = 5.f;  //标记背景变化范围，超过该范围的属于别的物体进入
	float thres_low = 5.f;
	std::vector<cv::Mat> all_mats;
	std::vector<cv::Mat> high_channls; //全部通道的变化上限
	std::vector<cv::Mat> low_channls;  //全部通道的变化下线
	cv::Mat mat_avg;
	cv::Mat mat_diff;
public:
	BackGroundStudy();
	~BackGroundStudy();
	cv::Mat set_data(const std::string video_str, float high = 5.f, float low = 5.f, std::size_t index = 0;);
	void show_diff(const cv::Mat& src, cv::Mat& diff);
	void training(cv::Mat& train_high , cv::Mat& train_low);
};
```
该类构造函数和析构函数使用默认即可。set_data()函数用于设置模型训练需要的一些数据
```c++
cv::Mat BackGroundStudy::set_data(const std::string video_str, float high, float low, std::size_t index) {
	all_mats.clear();
    //设置背景变化，上限和下线默认值都是5
	thres_high = high;
	thres_low = low;

	cv::VideoCapture video;
	if (video.open(video_str))
		std::cout << "视频加载成功" << std::endl;
	else {
		std::cout << "视频加载失败" << std::endl;
		std::abort();
	}

	cv::Mat tmp,v;
	while (video.read(tmp)) {
        //因为稍后计算的需要，这里要把全部图片数据类型转化为64位浮点，这步可能很耗时间
		tmp.convertTo(v, CV_64F, 1.0 / 255.0);
		all_mats.push_back(v.clone());
	}

	std::cout << "已经成功载入视频中的" << all_mats.size() << "帧图片" << std::endl;

	cv::Size size = all_mats[0].size();
	mat_avg = cv::Mat::zeros(size, CV_64FC3);
	mat_diff = cv::Mat::zeros(size, CV_64FC3);

    //返回视频的一帧图片，学习完成后可以使用这帧图片和学习数据对比，看是否有物体进入
	if (index < 0 || index >= all_mats.size())
		index = all_mats.size() - 1;
	return all_mats[index];
}
```
接下来是training()函数
```c++
void BackGroundStudy::training(cv::Mat& train_high , cv::Mat& train_low) {
	cv::Mat tmp, mat_prev;
	std::size_t cs = all_mats.size();
	for (std::size_t i = 0; i < cs; ++i) {
        //将全部图片累加
		mat_avg += all_mats[i];

		if (i != 0) {
            //将图片的差值累加
			cv::absdiff(mat_prev, all_mats[i], tmp);
			mat_diff += tmp;
		}
		mat_prev = all_mats[i];
	}

    //计算出全部图片的平均值和全部差值的平均值
	mat_avg /= static_cast<float>(cs);
	mat_diff /= static_cast<float>(cs - 1);

    //设置范围，平均值+平均差x权重 为变化的上限
    //平均值-平均差x权重 为变化的下线
	mat_high = mat_avg + mat_diff * thres_high;
	mat_low = mat_avg - mat_diff * thres_low;

	train_high = mat_high;
	train_low = mat_low;

    //将上下限每个通道单独保存，如果一帧画面中，一个通道的部分元素变化过大，就可以认为有物体进入
	cv::split(mat_high, high_channls);
	cv::split(mat_low, low_channls);
}
```
training()函数的原理很简单，就是先求全部样本的平均值，再把每帧图片的差值相加并求得平均差值，然后根据权重计算得到变化的上下限。这里有个细节，计算平均值的时候需要用到Mat除以数量，由于我的编译器是64位的，因此在set_data()函数中将图片的全部数据类型转换成了CV_64F，如果你用的是32位编译器，也可以把图全部设置位CV_32F。

最后是对比函数，将图片和训练后比较
```c++
void BackGroundStudy::show_diff(const cv::Mat& src, cv::Mat& diff) {
	std::vector<cv::Mat> src_channls;
	cv::split(src, src_channls);

	diff = cv::Mat::zeros(src.size(), CV_8UC1);
	cv::Mat tmp0,tmp1,tmp2;
	cv::inRange(src_channls[0], low_channls[0], high_channls[0], tmp0);
	cv::inRange(src_channls[1], low_channls[1], high_channls[1], tmp1);
	cv::inRange(src_channls[2], low_channls[2], high_channls[2], tmp2);
	//将需要对比的图片，每个通道和学习样本对比，标记处每个通道范围内和范围外的元素
	cv::max(tmp0, diff, diff);
	cv::max(tmp1, diff, diff);
	cv::max(tmp2, diff, diff);
}
```
这个函数将输入图片src的三个通道和学习后的样本逐一对比，超过范围的元素就设置为255，三次对比后三个通道所有超过范围的元素都会显示在参数diff上。

完成了这样一个学习模型后，可以选择一段视频来进行测试，下列代码中的tree.avi视频是从opencv安装目录下的~/opencv/sources/samples/data目录里复制出来的
```c++
int main() {
	BackGroundStudy backgroundstudy;
	cv::Mat mat_dst;

	//输入视频作为学习样本，并随机选择其中一帧作为和样本的对比，这里我随机选择了第54
	cv::Mat mat_first = backgroundstudy.set_data("E:/BackGround/tree.avi",5.f,5.f,54);
	cv::Mat mat_train_high,mat_train_low;
	backgroundstudy.training(mat_train_high, mat_train_low);
	backgroundstudy.show_diff(mat_first, mat_dst);

	cv::imshow("视频随机帧", mat_first);
	cv::imshow("HIGH", mat_train_high);
	cv::imshow("LOW", mat_train_low);
	cv::imshow("背景差", mat_dst);
	cv::waitKey();
	cv::destroyAllWindows();
}
```
训练结果为

![图2](https://jxf2008-1302581379.cos.ap-nanjing.myqcloud.com/github_blog/opencv/background_2.png)

随机选择的一帧图片和学习样本进行的对比的结果为：

![图3](https://jxf2008-1302581379.cos.ap-nanjing.myqcloud.com/github_blog/opencv/background_3.png)

平均背景法的缺点在于，采用该方法时，背景的变化不能太大。比如一个对着某个以一定速度旋转的风扇，如果风扇叶片和叶片之间的差距比较大，采用平均背景法来过滤背景时，就会导致风扇区域的区间非常大，如果有物体进入该区域，引起的变化可能小于背景本身，从而导致无法识别。另外一个问题是，平均背景法没有考虑光线变化的因素。

## CodeBook

码书（CodeBook）是另一种区分背景与前景的方法，其基本逻辑是基于图片中每一个点来进行判断。比如有一段视频，总计n幅图片，其图片的大小为100x100，那对于图片内任意一个点（x,y）来说，这段视频在这个坐标上总计有n个点。码书的是将视频中的相同位置的点进行对比的一中算法。

首先是点，对于一幅3通道图片来说，码书对于点的概念可以理解为“盒子”，盒子有核心和边界，而核心和边界都有上下限，而边界则是核心外围的区域，核心和外围的初始距离可以根据实际情况修改，举例：假设图片上坐标为（x,y）的点为（100，150，200）,那么该点核心上限为（（100，150，200），下限也同样为（100，150，200），核心和外围的初始距离设为RANGE = 10，那该点的边界上限为（100+RANGE，150+RANGE，200+RANGE），既（110，160，210），边界下限为（100-RANGE，150-RANGE，200-RANGE），既（90，140，190）；

码书的概念就是某个坐标的全部盒子的合集，码书算法会将全部盒子进行逐一对比，判断是否命中。假设盒子A和盒子B进行对比，则命中的概念如下：

1. B的核心位于A的A的边界范围内，则将B的核心和A的核心比较，取最大值作为A的核心，同时将B从码书中移除，而A的被命中次数+1；举例：A的核心上限和下限均为（100，150，200），而边界上限为（110，160，210），下限为（90，140，190），而B的核心下限为（98，149，202），B的核心上限为（109，155，208）B的核心满足
```c++
B.core_low[0] > A.border_low[0] && B.core_high[0] < A.border_high[0];
B.core_low[1] > A.border_low[1] && B.core_high[1] < A.border_high[1];
B.core_low[2] > A.border_low[2] && B.core_high[2] < A.border_high[1];
```
则认为A被B命中，此时A的核心下限变为（98，149，200），A的核心上限变为（109，155，208），同时B从码书中移除

2. 如果B的核心位于A的边界外，但B的边界上限位于A的边界上限内（既B的边界上限小于A的边界上限），则将A的边界上限+1，如果B的边界下限也位于A的边界下限内（既B的边界下限大于A的边界下限），则将A的边界下限-1。B则保留在码书内。

3. 如果B的核心位于A的边界外，同时B的边界也在A的边界外，B同样保留在码书内。

在完成了码书内全部的盒子对比后，会得到若干个盒子，这些盒子的核心和边界大小均有所不同，同时每个盒子的被命中次数也不一样，一般来说，被命中的次数越多，盒子的核心和边界范围就越大。而被命中次数较少的盒子可以被看做前景，将其从码书中移除，这样就可以得到背景的元素值的范围

此时已经完成了码书的初步训练，接下来只要把目标图片和码书对比，如果目标图片的元素在码书范围内，则可以认为该值为背景，否则为前景。

下面示例代码是演示如何通过码书来区分背景，首先是盒子代码
```c++
const int CHANNELS = 3;  //处理3通道图片
const uchar BORDER_RANGE = 10;//该值为核心到边界的默认距离

class CodeItem {
private:
	std::size_t hit_count = 0;//被命中次数
	uchar* core_high;
	uchar* core_low;
	uchar* border_high;
	uchar* border_low;
public:
	CodeItem() = default;
	CodeItem(const cv::Vec3b& p , uchar border = BORDER_RANGE);
	CodeItem(const CodeItem& v);
	CodeItem operator=(const CodeItem& v);
	~CodeItem();

	//判断是否被别的盒子命中
	bool hit_bull_eye(const CodeItem& item);

	//对比函数，用于判断是否位于盒子范围内
	bool in_range(const CodeItem& item, std::size_t offset_high = 0, std::size_t offset_low = 0);

	uchar get_core_high(std::size_t index)const { return *(core_high + index); }
	uchar get_core_low(std::size_t index)const { return *(core_low + index); }
	uchar get_border_high(std::size_t index)const { return *(border_high + index); }
	uchar get_border_low(std::size_t index)const { return *(border_low + index); }
	std::size_t get_hit_count()const { return hit_count; }
};
```
```c++
CodeItem::CodeItem(const cv::Vec3b& p, uchar border):
	core_high(new uchar[CHANNELS]),
	core_low(new uchar[CHANNELS]),
	border_high(new uchar[CHANNELS]),
	border_low(new uchar[CHANNELS]) {
	for (std::size_t i = 0; i < CHANNELS; ++i) {
		*(core_high + i) = p[i];
		*(core_low + i) = p[i];
		*(border_high + i) = (p[i] + border) > 255 ? 255 : (p[i] + border);
		*(border_low + i) = (p[i] - border) < 0 ? 0 : (p[i] - border);
	}
}

CodeItem::CodeItem(const CodeItem& v):
	core_high(new uchar[CHANNELS]),
	core_low(new uchar[CHANNELS]),
	border_high(new uchar[CHANNELS]),
	border_low(new uchar[CHANNELS]) {
	
	for (std::size_t i = 0; i < CHANNELS; ++i) {
		*(core_high + i) = *(v.core_high + i);
		*(core_low + i) = *(v.core_low + i);
		*(border_high + i) = *(v.border_high + i);
		*(border_low + i) = *(v.border_low + i);
	}

	hit_count = v.hit_count;
}

CodeItem CodeItem::operator=(const CodeItem& v) {
	for (std::size_t i = 0; i < CHANNELS; ++i) {
		*(core_high + i) = *(v.core_high + i);
		*(core_low + i) = *(v.core_low + i);
		*(border_high + i) = *(v.border_high + i);
		*(border_low + i) = *(v.border_low + i);
	}

	hit_count = v.hit_count;

	return *this;
}

CodeItem::~CodeItem() {
	delete[] core_high;
	delete[] core_low;
	delete[] border_high;
	delete[] border_low;
}

bool CodeItem::hit_bull_eye(const CodeItem& item) {
	//检测点item是否位于边界之内,既是否命中
	std::size_t nu = 0;
	for (std::size_t i = 0; i < CHANNELS; ++i) {
		uchar h = item.get_core_high(i);
		uchar l = item.get_core_low(i);
		if (l >= border_low[i] && h <= border_high[i])
			++nu;
	}
	
	//如果命中，就把core扩大范围
	if (nu == CHANNELS) {
		++hit_count;
		for (std::size_t i = 0; i < CHANNELS; ++i) {
			uchar h = item.get_core_high(i);
			uchar l = item.get_core_low(i);
			if (*(core_low + i) > l)
				* (core_low + i) = l;
			if (*(core_high + i) < h)
				* (core_high + i) = h;
		}
	}

	//如果没有命中，但位于元素的范围内，则border+1
	if (nu != CHANNELS) {
		for (std::size_t i = 0; i < CHANNELS; ++i) {
			uchar h = item.get_border_high(i);
			uchar l = item.get_border_low(i);
			if (*(border_high + i) < h)
				* (border_high + i) += 1;
			if (*(border_low + i) > l)
				* (border_low + i) -= 1;
		}
	}

	return nu == CHANNELS;
}

//在判断元素是否位于盒子核心内时，任然添加了一个偏移量，如果元素位于盒子不太远出
//则可以判断任然在范围内，这个偏移量需要根据实际情况逐一测试来确定
bool CodeItem::in_range(const CodeItem& item, std::size_t offset_high, std::size_t offset_low) {
	std::size_t nu = 0;
	for (std::size_t i = 0; i < CHANNELS; ++i) {
		uchar h = item.get_core_high(i);
		uchar l = item.get_core_low(i);
		if (l > (border_low[i]- offset_low) && h < (border_high[i] + offset_high))
			++nu;
	}
	return nu == CHANNELS;
}
```
接下来是码书的实现
```c++
class CodeBook{
private:
	std::vector<CodeItem> code_item;
public:
	CodeBook() = default;
	CodeBook(const std::vector<cv::Vec3b>& points, uchar border = BORDER_RANGE);
	~CodeBook();
	void update_codebook();
	
	void clear_stale(std::size_t prop = 5);
	bool in_range(const cv::Vec3b& p ,std::size_t offset_high = 0, std::size_t offset_low = 0)const;
};
```
```c++
CodeBook::CodeBook(const std::vector<cv::Vec3b>& points,uchar border) {
	for (std::size_t i = 0; i < points.size(); ++i)
		code_item.push_back(CodeItem(points.at(i),border));
}

CodeBook::~CodeBook() {

}

void CodeBook::update_codebook() {
	std::vector<CodeItem> train_dst;
	train_dst.push_back(code_item.at(0));
	//将每个点和train_dst内的元素对比，被命中的元素扩大核范围

	for (std::size_t i = 1; i < code_item.size(); ++i) {
		bool hit_successful = false;
		for (std::size_t n = 0; n < train_dst.size(); ++n) {
			if (train_dst[n].hit_bull_eye(code_item.at(i))) {
				hit_successful = true;
				break;
			}
		}
		//没有命中任何元素的点加入train_dst
		if (!hit_successful) 
			train_dst.push_back(code_item.at(i));
	}

	code_item.clear();
	for (auto A : train_dst)
		code_item.push_back(A);
}

//该函数用于清楚被命中次数较少的盒子，默认移除被命中次数地狱最高值一般的盒子
void CodeBook::clear_stale(std::size_t prop) {
	std::size_t max_nu = 0;
	for (auto A : code_item) {
		if (max_nu < A.get_hit_count())
			max_nu = A.get_hit_count();
	}

	if (prop <= 0 || prop >= 10)
		prop = 5;
	std::size_t line = max_nu / 10 * prop;
	//移除被命中次数较少的元素

	std::vector<CodeItem> tmp;
	for (auto A : code_item) {
		if (A.get_hit_count() > line)
			tmp.push_back(A);
	}

	code_item.clear();
	for (auto A : tmp)
		code_item.push_back(A);
}

bool CodeBook::in_range(const cv::Vec3b & p, std::size_t offset_high, std::size_t offset_low)const {
	bool fg = false;
	for (auto A : code_item) {
		if (A.in_range(p, offset_high, offset_low)) {
			fg = true;
			break;
		}
	}
	return fg;
}
```
接下啦是构建一个基本的码书学习模型，码书是一段视频中，一个相同坐标的合集，因此一段视频的码书数量等于视频的大小
```c++
class CodeBookModel{
private:
	std::vector<cv::Mat> mat_from_video;
	std::vector<CodeBook> codebooks;
public:
	CodeBookModel() = default;
	CodeBookModel(const std::string& video_path, uchar border = BORDER_RANGE);
	~CodeBookModel() = default;
	cv::Mat get_video_pix(std::size_t index)const { return mat_from_video.at(index); }
	void train(std::size_t prop = 5);
	void show_diff(const cv::Mat& src, cv::Mat& dst, std::size_t offset_high = 0, std::size_t offset_low = 0);
};
```
```c++
//构造函数打开视频，并读取视频中的全部图片
CodeBookModel::CodeBookModel(const std::string& video_path, uchar border) {
	cv::VideoCapture video(video_path);
	if (video.isOpened())
		std::cout << "视频打开成功" << std::endl;
	cv::Mat tmp;
	while (video.read(tmp))
		mat_from_video.push_back(tmp.clone());

	cv::Size video_size = mat_from_video.at(0).size();
	std::size_t x = video_size.width;
	std::size_t y = video_size.height;

    //图片中的每个相同坐标的点的合集构建成一个码书
	for (std::size_t h = 0; h < y; ++h) {
		for (std::size_t w = 0; w < x; ++w){
			std::vector<cv::Vec3b> tmp_points;
			for (auto A : mat_from_video) 
				tmp_points.push_back(A.at<cv::Vec3b>(h * x + w));
			codebooks.push_back(CodeBook(tmp_points, border));
		}
	}
}

//训练函数，先对比码书，然后移除被命中次数较少的盒子
void CodeBookModel::train(std::size_t prop) {
	int i = 0;
	for (std::size_t i = 0; i < codebooks.size(); ++i) {
		codebooks[i].update_codebook();
		codebooks[i].clear_stale(prop);
		++i;
	}
}

//将图片和学习完的码书模型对比
void CodeBookModel::show_diff(const cv::Mat& src, cv::Mat& dst, std::size_t offset_high, std::size_t offset_low) {
	dst = cv::Mat::zeros(src.size(), CV_8UC1);

	std::size_t x = src.size().width;
	std::size_t y = src.size().height;

	for (std::size_t h = 0; h < y; ++h) {
		for (std::size_t w = 0; w < x; ++w) {
			if (codebooks[h * x + w].in_range(src.at<cv::Vec3b>(h * x + w), offset_high, offset_low))
			    dst.at<uchar>(h * x + w) = 255;
		}
	}
}
```
最后是调用该码书模型，
```c++
int main() {
	cv::Mat mat_dst;
	//任然采用opencv自带的示例视频
	CodeBookModel codebookmodel("E:/BackGround/tree.avi");

	//这里任然抓取第54帧做对比，这样可以和之前的平均背景法对比效果
	cv::Mat mat_54 = codebookmodel.get_video_pix(54);
	codebookmodel.train();

	//在对比的时候，偏移量设为10，这个值通常需要大量测试来确定
	//因为这里只是示例代码，因此只是随便将其设置为10
	codebookmodel.show_diff(mat_54, mat_dst,10,10);

	cv::imshow("随机帧", mat_54);
	cv::imshow("区别", mat_dst);

	cv::waitKey();
	cv::destroyAllWindows();
}
```
最后结果为

![图4](https://jxf2008-1302581379.cos.ap-nanjing.myqcloud.com/github_blog/opencv/background_4.png)